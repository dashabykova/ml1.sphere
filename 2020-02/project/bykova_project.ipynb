{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from sklearn import decomposition, manifold\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "import pymorphy2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обрабатываем странички: используем title, keywords, desc, refs, \n",
    "#оставляем только русские слова длинее 3 букв, приводим все слова к нормальной форме\n",
    "def russian_words(s):\n",
    "    new_s = ''\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    for w in re.findall(\"[А-я]+\", s):\n",
    "        if len(w) >= 3:\n",
    "            new_s = new_s + ' ' + morph.parse(w.lower())[0].normal_form\n",
    "    #[morph.parse(word)[0].normal_form for word in new_s.split() if len(word) >= 3]\n",
    "    return new_s\n",
    "\n",
    "def smart_process_page(page):\n",
    "    info = {}\n",
    "    ident = re.search(r'[0-9]+', page)\n",
    "    info['id'] = ident.group(0)\n",
    "    with codecs.open(page, 'r', 'utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        title = soup.title.text if soup.title else ''\n",
    "        info['title'] = russian_words(title)\n",
    "        ref = [russian_words(i.text.strip()) for i in soup.find_all('a')]\n",
    "        info['refs'] = ';'.join(list(filter(lambda x: x != '', ref)))\n",
    "        meta = soup.find_all('meta')\n",
    "        info['desc'] = ''\n",
    "        info['keywords'] = ''\n",
    "        for tag in meta:\n",
    "            if 'name' in tag.attrs:\n",
    "                if tag.attrs['name'] == 'keywords':\n",
    "                    keywords = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['keywords'] = russian_words(keywords)\n",
    "                if tag.attrs['name'] == 'description':\n",
    "                    desc = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['desc'] = russian_words(desc)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normal_form_pages_info.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('normal_form_pages_info.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_data = {}\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = df[df.id == doc_id].title.values\n",
    "    refs = df[df.id == doc_id].refs.values\n",
    "    keywords = df[df.id == doc_id].keywords.values\n",
    "    desc = df[df.id == doc_id].desc.values\n",
    "    #print(doc_id, title, refs, keywords, desc, target)\n",
    "    if doc_group not in traingroups_data:\n",
    "        traingroups_data[doc_group] = []\n",
    "    traingroups_data[doc_group].append((doc_id, title[0], refs[0], keywords[0], desc[0], target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    if doc_group not in traingroups:\n",
    "        traingroups[doc_group] = []\n",
    "    traingroups[doc_group].append([doc_id,target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccar(s1, s2):\n",
    "    u = len(s1.union(s2))\n",
    "    if u > 0:\n",
    "        return len(s1.intersection(s2))/u\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s):\n",
    "    #temp = s.strip(' \\t\\n!()#*?:;,.').split()\n",
    "    #return [morph.parse(word)[0].normal_form for word in s.split() if len(word) >= 3]\n",
    "    #return [i.strip('.,:').lower() for i in temp if len(i) >= 3]\n",
    "    return s.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in tqdm(traingroups_data):\n",
    "    docs = traingroups_data[new_group]\n",
    "    for k, (doc_id, title, refs, keywords, desc, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        dist = {'title':[], 'keywords':[], 'desc':[], 'title_jaccar':[], 'refs_jaccar':[]}\n",
    "        words = set(bag_of_words(title))\n",
    "        wdesc = set(bag_of_words(desc))\n",
    "        wkey = set(bag_of_words(keywords))\n",
    "        temp = [i.strip().split() for i in refs.split(';') if len(i) >= 3]\n",
    "        wrefs = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) >= 3])\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, refs_j, keywords_j, desc_j, target_j = docs[j]\n",
    "            words_j = set(bag_of_words(title_j))\n",
    "            dist['title'].append(len(words.intersection(words_j)))\n",
    "            wdesc_j = set(bag_of_words(desc_j))\n",
    "            wkey_j = set(bag_of_words(keywords_j))\n",
    "            temp = [i.strip().split() for i in refs_j.split(';') if len(i) >= 3]\n",
    "            wrefs_j = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) >= 3])\n",
    "            dist['keywords'].append(len(wkey.intersection(wkey_j)))\n",
    "            dist['desc'].append(len(wdesc.intersection(wdesc_j)))\n",
    "            dist['title_jaccar'].append(jaccar(words, words_j))\n",
    "            dist['refs_jaccar'].append(jaccar(wrefs_j, wrefs))\n",
    "        features = []\n",
    "        for key in ['title', 'keywords', 'desc', 'title_jaccar', 'refs_jaccar']:\n",
    "            if (key == 'keywords') | (key == 'desc'):\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:4])\n",
    "            else:\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:15])\n",
    "        X_train.append(features)\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение моделей, подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция, с помощью которой можно тестировать разные модели\n",
    "def cross_validation(traingroups_data, X_train, y_train, groups_train, model='logreg', C=0.11, \n",
    "                     m_depth=6, m_features=4, ridge=False, threshold=0.5):\n",
    "    parameters_f1 = {}\n",
    "    for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "        X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "        y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "        X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "        y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "        if model == 'logreg':\n",
    "            clf = LogisticRegression(C = C, solver='lbfgs')\n",
    "        elif model == 'linreg':\n",
    "            clf = LinearRegression()\n",
    "            if ridge:\n",
    "                clf = Ridge(alpha=C)\n",
    "        elif model == 'rf':\n",
    "            clf = RandomForestClassifier(max_depth=m_depth, max_features = m_features, n_estimators = 20, random_state=0)\n",
    "        clf.fit(X_train_temp, y_train_temp)\n",
    "        prediction = clf.predict(X_val)\n",
    "        if model == 'linreg':\n",
    "            prediction = np.asarray(prediction >= threshold, dtype = int)\n",
    "        f1 = f1_score(y_val, prediction)\n",
    "        fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "        fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "        if (C, m_depth, m_features, ridge, threshold) not in parameters_f1:\n",
    "            parameters_f1[(C, m_depth, m_features, ridge, threshold)] = []\n",
    "        parameters_f1[(C, m_depth, m_features, ridge, threshold)].append((f1, fbeta_1, fbeta_2))\n",
    "    return parameters_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве финальной модели для предсказания была выбрана линейная регрессия с добавлением признаков \"вероятность, предсказанная логрегрессией\", \"вероятность предсказанная случайным лесом\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_data = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = df[df.id == doc_id].title.values\n",
    "    refs = df[df.id == doc_id].refs.values\n",
    "    keywords = df[df.id == doc_id].keywords.values\n",
    "    desc = df[df.id == doc_id].desc.values\n",
    "    if doc_group not in testgroups_data:\n",
    "        testgroups_data[doc_group] = []\n",
    "    testgroups_data[doc_group].append((doc_id, title[0], refs[0], keywords[0], desc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for new_group in tqdm(testgroups_data):\n",
    "    docs = testgroups_data[new_group]\n",
    "    for k, (doc_id, title, refs, keywords, desc) in enumerate(docs):\n",
    "        dist = {'title':[], 'keywords':[], 'desc':[], 'title_jaccar':[], 'refs_jaccar':[]}\n",
    "        words = set(bag_of_words(title))\n",
    "        wdesc = set(bag_of_words(desc))\n",
    "        wkey = set(bag_of_words(keywords))\n",
    "        temp = [i.strip().split() for i in refs.split(';') if len(i) > 4]\n",
    "        wrefs = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) > 3])\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, refs_j, keywords_j, desc_j = docs[j]\n",
    "            words_j = set(bag_of_words(title_j))\n",
    "            dist['title'].append(len(words.intersection(words_j)))\n",
    "            wdesc_j = set(bag_of_words(desc_j))\n",
    "            wkey_j = set(bag_of_words(keywords_j))\n",
    "            temp = [i.strip().split() for i in refs_j.split(';') if len(i) > 4]\n",
    "            wrefs_j = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) > 3])\n",
    "            dist['keywords'].append(len(wkey.intersection(wkey_j)))\n",
    "            dist['desc'].append(len(wdesc.intersection(wdesc_j)))\n",
    "            dist['title_jaccar'].append(jaccar(words, words_j))\n",
    "            dist['refs_jaccar'].append(jaccar(wrefs_j, wrefs))\n",
    "        features = []\n",
    "        for key in ['title', 'keywords', 'desc', 'title_jaccar', 'refs_jaccar']:\n",
    "            if (key == 'keywords') | (key == 'desc'):\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:4])\n",
    "            else:\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:15])\n",
    "        #features.append(isoutlier_t[(new_group, doc_id)])\n",
    "        X_test.append(features)\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "prediction = logreg.predict_proba(X_test)[:, 1]\n",
    "prediction = (prediction - np.mean(prediction))/np.std(prediction)\n",
    "rf = RandomForestClassifier(max_depth=8, max_features = 4, n_estimators = 100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_prediction = rf.predict_proba(X_test)[:, 1]\n",
    "rf_prediction = (rf_prediction - np.mean(rf_prediction))/np.std(rf_prediction)\n",
    "X_test = np.hstack([X_test, prediction.reshape(-1,1)])\n",
    "X_test = np.hstack([X_test, rf_prediction.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "x1 = X_train[:int(X_train.shape[0]/2), :]\n",
    "y1 = y_train[:int(X_train.shape[0]/2)]\n",
    "x2 = X_train[int(X_train.shape[0]/2):, :]\n",
    "y2 = y_train[int(X_train.shape[0]/2):]\n",
    "logreg.fit(x1, y1)\n",
    "pred1 = logreg.predict_proba(x2)[:, 1]\n",
    "#pred1 = (pred1 - np.mean(pred1))/np.std(pred1)\n",
    "logreg.fit(x2, y2)\n",
    "pred2 = logreg.predict_proba(x1)[:, 1]\n",
    "#pred2 = (pred2 - np.mean(pred2))/np.std(pred2)\n",
    "pred = np.concatenate([pred2, pred1])\n",
    "pred = (pred - np.mean(pred))/np.std(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, pred.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred = clf.predict(X_test)\n",
    "finalpred_t = np.asarray(finalpred >= 0.375, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred = pd.DataFrame(data = {'pair_id':test_data.pair_id, 'target': finalpred_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred.to_csv('linear_rf_log__comb_submission.сsv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
