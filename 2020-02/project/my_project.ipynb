{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from sklearn import decomposition, manifold\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:Пятиэтажки сносимых серий — Комплекс градостроительной политики и строительства города Москвы\n",
      "keywords:снос пятиэтажек\n",
      "desc:\n",
      "ref:['', '', '', 'Комплекс градостроительной политики и строительства города Москвы', '', '', '', '', '', '✕', 'Метро', 'Дороги', 'Новая Москва', 'МЦК', 'Карта строек', 'Округа', 'Госпрограммы', 'О Стройкомплексе', 'Жителям', 'Градостроителям', 'Застройщикам', 'СМИ', 'Контакт-центр', 'Деятельность', 'Структура', 'Справочник организаций', 'Контакты', 'Законы, постановления, распоряжения, указы', 'Проекты правовых нормативных актов', 'Решения об утверждении проектной документации', 'Государственные программы', 'Строительство в округах, районах', 'Архитектурные конкурсы', 'Все стройки Москвы', 'Карты развития дорожно-транспортной инфраструктуры', 'Строительство жилья', 'Снос пятиэтажек', 'Строительство поликлиник в Москве', 'Строительство детских садов', 'Строительство школ и БНК', 'Стадионы Москвы', 'Реновация промзон', 'Долевое строительство', 'Уникальная архитектура', 'Фотоленты', 'Перепланировка квартир', 'Центральный детский магазин на Лубянке', 'Долгострои', 'Реконструкция советских кинотеатров', 'Парк «Зарядье»', 'Москва-Сити', 'Отраслевые схемы', 'ИСОГД', 'МТСК', 'Генеральный план развития Москвы', 'Строительная наука', 'СРО', 'Городские конкурсы', 'Новые правила размещения информационных вывесок на улицах Москвы', 'Информационное письмо', 'Совет молодых специалистов', 'Калькулятор процедур', 'Кабинет застройщика', 'Электронные услуги', 'Рейтинг Всемирного банка', 'Инвестиционный портал Москвы', 'Оформление стройплощадок', 'МИПИМ', 'Пресс-релизы', 'Фотогалерея', 'Видеогалерея', 'Статьи', 'Интервью', 'Инфографика', 'Стенограммы', 'Пресс-служба', 'Какие дома построят для переселения из пятиэтажек?', 'Стадион «Лужники»: до и после реконструкции', 'Реконструкция стадиона «Лужники» завершена', 'Как реконструировали главную спортивную арену России', 'Здание-«лотос» в Китае', 'Открыт участок Северо-Западной хорды', 'Буклет с примерами отделки квартир', 'Как строят панельные дома комфорт-класса', 'Переуплотнять город в рамках реновации  не будем', 'Качество строительства жилья в Москве', 'Предварительный список пятиэтажек под снос', 'Главная', 'Снос пятиэтажек в Москве', 'Снос пятиэтажек', 'Пятиэтажки сносимых серий', '', 'Метро', 'Дороги', 'Новая Москва', 'МЦК', 'Карта строек', 'Округа', 'Госпрограммы', '', '', 'Все новости', '58 пятиэтажек осталось разобрать в Москве', 'Невостребованные программой реновации площадки могут выставить на торги - Хуснуллин', 'В Москве снесли очередную пятиэтажку', 'Власти Москвы помогут жителям сносимых пятиэтажек переехать в новые дома', 'В Москве осталось снести 60 пятиэтажек', 'В Москве останется разобрать 14 пятиэтажек  первой «волны» сноса в 2018 году - заммэра', 'Программа реновации улучшит транспортную ситуацию в Москве - Хуснуллин', 'В Москве осталось снести 61 пятиэтажку', 'будет готов к лету', '', 'Дома серии 1605-АМ', 'Дома серии 1МГ-300 (МГ-300)', 'Дома серии II-32', 'Дома серии II-35', 'Дома серии К-7', 'Снос пятиэтажек', '', '', 'Все новости', 'Москомархитектура представит концепцию развития территории вокруг МЦК', 'Участок Второго кольца метро от «Авиамоторной» до «Нижней Масловки» запустят в 2019 году', 'Лучшие дизайн-проекты трех новых станций метро выбрали в «Активном гражданине»', 'В парке «Зарядье» начали благоустраивать пешеходное пространство', 'Более 250 км дорог построят в Москве в ближайшие два года', 'Пустыри в Москве исчезнут после программы реновации — Кузнецов', 'Отель откроют в офисном здании 1917 года постройки', 'На месте стоянки в промзоне «Калошино» появится автомастерская', 'Все стройки Москвы', 'Снос пятиэтажек', 'Контакт-центр', 'Карты развития дорожно-транспортной инфраструктуры', 'Строительство новых поликлиник', 'Проекты детских садов, школ и БНК', 'Участникам долевого строительства', 'Реновация промзон', 'Жилье', 'Типовые варианты перепланировки квартир', 'Общая информация', 'Градостроительная политика', 'Жилище', 'Адресная инвестиционная программа', 'СРО', 'Строительная наука', 'Городские конкурсы', 'Архитектурные конкурсы', 'Электронные услуги', 'Законы, постановления, распоряжения, указы', 'Проекты правовых нормативных актов', 'Решения об утверждении проектной документации', 'Перспективная карта метро', 'Северо-Западная хорда', 'Строительство хорд', 'Реконструкция вылетных магистралей', 'Строительство мостов и эстакад', 'Как возводят эстакады', 'Как строят автомобильные тоннели', 'Виртуальная выставка проектов', '12 точек роста «новой Москвы»', 'Конкурс на концепцию развития', 'Строительство новых социальных объектов', 'Итоги работы за год', 'Новости', 'Пресс-релизы', 'Фотогалерея', 'Видеогалерея', 'Статьи', 'Интервью', 'Структура', 'Справочник организаций', 'Карта сайта', 'Сообщить об ошибке', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "path = 'content.tar/content/'\n",
    "filename = '1115.dat'\n",
    "desc = ''\n",
    "keywords = ''\n",
    "with codecs.open(path + filename, 'r', 'utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    title = soup.title.text\n",
    "    meta = soup.find_all('meta')\n",
    "    ref = [i.text.strip() for i in soup.find_all('a')]\n",
    "    for tag in meta:\n",
    "        if 'name' in tag.attrs:\n",
    "            if tag.attrs['name'] == 'keywords':\n",
    "                keywords = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "            if tag.attrs['name'] == 'description':\n",
    "                desc = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "    text = soup.get_text()\n",
    "    print('title:{0}\\nkeywords:{1}\\ndesc:{2}\\nref:{3}'.format(title, keywords, desc, ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция, которая использовалась для извлечения информации на первом этапе\n",
    "def process_page(page):\n",
    "    info = {}\n",
    "    ident = re.search(r'[0-9]+', page)\n",
    "    info['id'] = ident.group(0)\n",
    "    with codecs.open(page, 'r', 'utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        title = soup.title.text if soup.title else ''\n",
    "        info['title'] = title\n",
    "        ref = [i.text.strip() for i in soup.find_all('a')]\n",
    "        info['refs'] = ';'.join(ref)\n",
    "        meta = soup.find_all('meta')\n",
    "        info['desc'] = ''\n",
    "        info['keywords'] = ''\n",
    "        for tag in meta:\n",
    "            if 'name' in tag.attrs:\n",
    "                if tag.attrs['name'] == 'keywords':\n",
    "                    keywords = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['keywords'] = keywords\n",
    "                if tag.attrs['name'] == 'description':\n",
    "                    desc = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['desc'] = desc\n",
    "    return info\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#улучшенная функция, которая использовалась для извлечения информации (с нормализацией текста)\n",
    "def russian_words(s):\n",
    "    new_s = ''\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    for w in re.findall(\"[А-я]+\", s):\n",
    "        if len(w) >= 3:\n",
    "            new_s = new_s + ' ' + morph.parse(w.lower())[0].normal_form\n",
    "    #[morph.parse(word)[0].normal_form for word in new_s.split() if len(word) >= 3]\n",
    "    return new_s\n",
    "\n",
    "def smart_process_page(page):\n",
    "    info = {}\n",
    "    ident = re.search(r'[0-9]+', page)\n",
    "    info['id'] = ident.group(0)\n",
    "    with codecs.open(page, 'r', 'utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        title = soup.title.text if soup.title else ''\n",
    "        info['title'] = russian_words(title)\n",
    "        ref = [russian_words(i.text.strip()) for i in soup.find_all('a')]\n",
    "        info['refs'] = ';'.join(list(filter(lambda x: x != '', ref)))\n",
    "        meta = soup.find_all('meta')\n",
    "        info['desc'] = ''\n",
    "        info['keywords'] = ''\n",
    "        for tag in meta:\n",
    "            if 'name' in tag.attrs:\n",
    "                if tag.attrs['name'] == 'keywords':\n",
    "                    keywords = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['keywords'] = russian_words(keywords)\n",
    "                if tag.attrs['name'] == 'description':\n",
    "                    desc = tag.attrs['content'] if 'content' in tag.attrs else ''\n",
    "                    info['desc'] = russian_words(desc)\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc60d0d3d2da43debb8aa4f861714ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = list(map(process_page, tqdm(glob.glob('content.tar/content/*'))))\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcf08c534d440278087883330da00ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = list(map(smart_process_page, tqdm(glob.glob('content.tar/content/*'))))\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05ffb45aef341fea107f9ca43501f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = list(map(smart_process_page, tqdm(glob.glob('content.tar/content/*'))))\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>refs</th>\n",
       "      <th>desc</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>аншина центр репродукция генетик фертимед москва</td>\n",
       "      <td>скачать; аншина центр репродукция генетик фер...</td>\n",
       "      <td>тип реферат размер резюме статья систематизир...</td>\n",
       "      <td>аншина центр репродукция генетик фертимед мос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>нужный помощь доска объявление заработать ден...</td>\n",
       "      <td>главный; регистрация; вход; главный страница;...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>курс валюта бобруйск хороший курс сегодня кур...</td>\n",
       "      <td>белорусский магазин дефицит видеокарта майнер...</td>\n",
       "      <td>курс валюта банка бобруйск обновление каждый ...</td>\n",
       "      <td>курс валюта бобруйск курс валюта курс валюта ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>как пользоваться компас леса рекомендация</td>\n",
       "      <td>жизнь; экономика; наука; авто; отдых; хай теч...</td>\n",
       "      <td>если собраться лес обязательно захватить себя...</td>\n",
       "      <td>как пользоваться компас леса как научиться по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>как удалить аккаунт</td>\n",
       "      <td>дробный; можно список ниже; войти; войти; пор...</td>\n",
       "      <td>подписываться канал ставить лайк писать позит...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0      1   аншина центр репродукция генетик фертимед москва   \n",
       "1     10   нужный помощь доска объявление заработать ден...   \n",
       "2    100   курс валюта бобруйск хороший курс сегодня кур...   \n",
       "3   1000          как пользоваться компас леса рекомендация   \n",
       "4  10000                                как удалить аккаунт   \n",
       "\n",
       "                                                refs  \\\n",
       "0   скачать; аншина центр репродукция генетик фер...   \n",
       "1   главный; регистрация; вход; главный страница;...   \n",
       "2   белорусский магазин дефицит видеокарта майнер...   \n",
       "3   жизнь; экономика; наука; авто; отдых; хай теч...   \n",
       "4   дробный; можно список ниже; войти; войти; пор...   \n",
       "\n",
       "                                                desc  \\\n",
       "0   тип реферат размер резюме статья систематизир...   \n",
       "1                                                      \n",
       "2   курс валюта банка бобруйск обновление каждый ...   \n",
       "3   если собраться лес обязательно захватить себя...   \n",
       "4   подписываться канал ставить лайк писать позит...   \n",
       "\n",
       "                                            keywords  \n",
       "0   аншина центр репродукция генетик фертимед мос...  \n",
       "1                                                     \n",
       "2   курс валюта бобруйск курс валюта курс валюта ...  \n",
       "3   как пользоваться компас леса как научиться по...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' аншина центр репродукция генетик фертимед москва',\n",
       "       ' нужный помощь доска объявление заработать деньга денежный помощь кредитный помощь',\n",
       "       ' курс валюта бобруйск хороший курс сегодня курс конверсия валюта',\n",
       "       ...,\n",
       "       ' сосед заливать мой балкон про остекление жилищный коммунальный хозяйство конференция юрклуб',\n",
       "       ' мамочка санкт петербург группа страна мама',\n",
       "       ' апноэ остановка сон причина академия вселенная счастие'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'refs'] = list(map(lambda x: x.encode('utf-8', 'replace').decode('utf-8'), df.loc[:,'refs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pages_info.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_pages_info.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('normal_form_pages_info.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерим признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('normal_form_pages_info.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28026 entries, 0 to 28025\n",
      "Data columns (total 5 columns):\n",
      "id          28026 non-null int64\n",
      "title       28026 non-null object\n",
      "refs        28026 non-null object\n",
      "desc        28026 non-null object\n",
      "keywords    28026 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>refs</th>\n",
       "      <th>desc</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>аншина центр репродукция генетик фертимед москва</td>\n",
       "      <td>скачать; аншина центр репродукция генетик фер...</td>\n",
       "      <td>тип реферат размер резюме статья систематизир...</td>\n",
       "      <td>аншина центр репродукция генетик фертимед мос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>нужный помощь доска объявление заработать ден...</td>\n",
       "      <td>главный; регистрация; вход; главный страница;...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>курс валюта бобруйск хороший курс сегодня кур...</td>\n",
       "      <td>белорусский магазин дефицит видеокарта майнер...</td>\n",
       "      <td>курс валюта банка бобруйск обновление каждый ...</td>\n",
       "      <td>курс валюта бобруйск курс валюта курс валюта ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>как пользоваться компас леса рекомендация</td>\n",
       "      <td>жизнь; экономика; наука; авто; отдых; хай теч...</td>\n",
       "      <td>если собраться лес обязательно захватить себя...</td>\n",
       "      <td>как пользоваться компас леса как научиться по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>как удалить аккаунт</td>\n",
       "      <td>дробный; можно список ниже; войти; войти; пор...</td>\n",
       "      <td>подписываться канал ставить лайк писать позит...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0      1   аншина центр репродукция генетик фертимед москва   \n",
       "1     10   нужный помощь доска объявление заработать ден...   \n",
       "2    100   курс валюта бобруйск хороший курс сегодня кур...   \n",
       "3   1000          как пользоваться компас леса рекомендация   \n",
       "4  10000                                как удалить аккаунт   \n",
       "\n",
       "                                                refs  \\\n",
       "0   скачать; аншина центр репродукция генетик фер...   \n",
       "1   главный; регистрация; вход; главный страница;...   \n",
       "2   белорусский магазин дефицит видеокарта майнер...   \n",
       "3   жизнь; экономика; наука; авто; отдых; хай теч...   \n",
       "4   дробный; можно список ниже; войти; войти; пор...   \n",
       "\n",
       "                                                desc  \\\n",
       "0   тип реферат размер резюме статья систематизир...   \n",
       "1                                                      \n",
       "2   курс валюта банка бобруйск обновление каждый ...   \n",
       "3   если собраться лес обязательно захватить себя...   \n",
       "4   подписываться канал ставить лайк писать позит...   \n",
       "\n",
       "                                            keywords  \n",
       "0   аншина центр репродукция генетик фертимед мос...  \n",
       "1                                                     \n",
       "2   курс валюта бобруйск курс валюта курс валюта ...  \n",
       "3   как пользоваться компас леса как научиться по...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь будут идеи про то, какие признаки использовать.\n",
    "- Число общих слов в заголовках в группе\n",
    "- Стандартное отклонение от (1)\n",
    "- Число общих слов в refs, keywords, desc\n",
    "- Косинусное расстояние между заголовками, refs, desc (среднее внутри группы)\n",
    "- Сложить всё в кучу и посчитать Жаккара\n",
    "- Лучше удалять знаки препинания и прочий мусор (мб регулярные выражения?) done\n",
    "- И вообще текст лучше нормализовать (чтобы окончания удалялись) done\n",
    "- является ли страничка выбросом с точки зрения dbscan\n",
    "\n",
    "Отбор признаков: считать корреляции с таргетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6552116552f3424ba893c17f5609c561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_data = {}\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = df[df.id == doc_id].title.values\n",
    "    refs = df[df.id == doc_id].refs.values\n",
    "    keywords = df[df.id == doc_id].keywords.values\n",
    "    desc = df[df.id == doc_id].desc.values\n",
    "    #print(doc_id, title, refs, keywords, desc, target)\n",
    "    if doc_group not in traingroups_data:\n",
    "        traingroups_data[doc_group] = []\n",
    "    traingroups_data[doc_group].append((doc_id, title[0], refs[0], keywords[0], desc[0], target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    if doc_group not in traingroups:\n",
    "        traingroups[doc_group] = []\n",
    "    traingroups[doc_group].append([doc_id,target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Песочница"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее идут пробы разных идей, можно не смотреть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.asarray(traingroups[1])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mtx = vectorizer.fit_transform(df[df.id.isin(indices)].title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mtx_theme= vectorizer.fit_transform(df[df.id.isin(theme)].title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer()\n",
    "hashing_mtx = hvectorizer.fit_transform(df[df.id.isin(indices)].title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<102x475 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, manifold\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_machine = decomposition.TruncatedSVD(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca_machine.fit_transform(tfidf_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03866454, 0.05067422, 0.04737904, 0.02871258, 0.02603472,\n",
       "       0.02292828, 0.02280408, 0.0201574 , 0.01756046, 0.01692816,\n",
       "       0.01665631, 0.01618035, 0.01521724, 0.01462557, 0.0144876 ,\n",
       "       0.01425048, 0.01374659, 0.01371677, 0.01344916, 0.01318795,\n",
       "       0.0130209 , 0.01265593, 0.01224448, 0.01175181, 0.01158525,\n",
       "       0.0114405 , 0.01096649, 0.01079836, 0.01047736, 0.01026153,\n",
       "       0.01003651, 0.0100155 , 0.00974265, 0.00979137, 0.00962768,\n",
       "       0.00977693, 0.00956313, 0.00968058, 0.00952478, 0.00941317,\n",
       "       0.00918494, 0.00903565, 0.00900045, 0.00892914, 0.00861795,\n",
       "       0.00833066, 0.00805761, 0.00805653, 0.00778934, 0.00770143])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_machine.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='complete', distance_threshold=0.85).fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.8, min_samples=2, metric='cosine').fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.labels_[same_theme_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(clustering.labels_ != -1, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_inside = pairwise_distances(X_pca, metric= 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9325404388768355"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dist_inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.63701717e-02, 1.65413213e-01, ...,\n",
       "        1.03769992e+00, 1.04601676e+00, 1.05050590e+00],\n",
       "       [0.00000000e+00, 3.34074536e-01, 3.34074536e-01, ...,\n",
       "        1.02772963e+00, 1.03194306e+00, 1.04141075e+00],\n",
       "       [0.00000000e+00, 3.71558454e-01, 5.63847098e-01, ...,\n",
       "        1.05096676e+00, 1.06504298e+00, 1.13861478e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 9.70880918e-01, 9.77676504e-01, ...,\n",
       "        1.02818470e+00, 1.03237017e+00, 1.23519531e+00],\n",
       "       [0.00000000e+00, 2.22044605e-16, 7.13807366e-02, ...,\n",
       "        1.02040012e+00, 1.02390340e+00, 1.02798458e+00],\n",
       "       [0.00000000e+00, 4.40214577e-01, 4.99175372e-01, ...,\n",
       "        1.05561723e+00, 1.07211250e+00, 1.07468742e+00]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(dist_inside, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_theme_indices = np.where(np.asarray(traingroups[1])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_theme_id = np.asarray(traingroups[1])[same_theme_indices][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([28, 34, 38, 41, 63, 82, 84, 98], dtype=int64),)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_theme_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84829034, 0.85253285, 0.86014829, 0.86410612, 0.84967683,\n",
       "       0.85582527, 0.83632201, 0.85058701])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dist_inside[same_theme_indices[0], :][:,same_theme_indices[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86156318, 0.9039839 , 0.97774935, 0.95611611, 0.94305625,\n",
       "       0.96962838, 0.96552611, 0.97548428])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dist_inside[same_theme_indices[0], :], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86266679, 0.89361704, 0.97247139, 0.89369979, 0.97518434,\n",
       "       0.96351423, 0.975296  , 0.88739875, 0.87301329, 0.9526289 ,\n",
       "       0.92737703, 0.97074684, 0.88403513, 0.91203252, 0.96284435,\n",
       "       0.90198509, 0.86104272, 0.8960462 , 0.93850382, 0.95314068,\n",
       "       0.93310311, 0.93820364, 0.91269946, 0.90316921, 0.91903961,\n",
       "       0.9414762 , 0.94810858, 0.91342309, 0.86156318, 0.97305285,\n",
       "       0.92804509, 0.95139299, 0.86741677, 0.88888789, 0.9039839 ,\n",
       "       0.97532251, 0.9063953 , 0.9075783 , 0.97774935, 0.98475879,\n",
       "       0.94871724, 0.95611611, 0.89615101, 0.95332946, 0.85264233,\n",
       "       0.95360734, 0.89662382, 0.95251703, 0.85899877, 0.93601977,\n",
       "       0.91435413, 0.95421151, 0.97664041, 0.92008978, 0.95954485,\n",
       "       0.9803891 , 0.96943491, 0.92301604, 0.85574101, 0.91571631,\n",
       "       0.9720689 , 0.9240197 , 0.93137361, 0.94305625, 0.93624256,\n",
       "       0.98309934, 0.9414576 , 0.92972871, 0.93613242, 0.95369368,\n",
       "       0.9651184 , 0.92375165, 0.88459262, 0.97246227, 0.93607308,\n",
       "       0.96850621, 0.96339771, 0.96276852, 0.9128178 , 0.95055389,\n",
       "       0.92152758, 0.95407   , 0.96962838, 0.90550526, 0.96552611,\n",
       "       0.91585867, 0.98375186, 0.9383466 , 0.94462385, 0.95354807,\n",
       "       0.86619297, 0.97287259, 0.89386764, 0.9088039 , 0.91481088,\n",
       "       0.98667948, 0.90466128, 0.96892448, 0.97548428, 0.97031914,\n",
       "       0.9173019 , 0.92942929])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dist_inside, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([28, 34, 38, 41, 63, 82, 84, 98], dtype=int64),)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_theme_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>refs</th>\n",
       "      <th>desc</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>15498</td>\n",
       "      <td>простой разборка ступица нива ремонт</td>\n",
       "      <td>ваза; войти; авто; бизнес; дом; интернет; мед...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6239</td>\n",
       "      <td>15613</td>\n",
       "      <td>снятие установка подшипник ступица нива нива</td>\n",
       "      <td>главный; ваза; ремонт; тюнинг; тест драйв; ха...</td>\n",
       "      <td>подшипник ступица лада амортизатор шаровой оп...</td>\n",
       "      <td>подвеска колесо нива пыльник рычаг амортизато...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6405</td>\n",
       "      <td>15763</td>\n",
       "      <td>замена регулировка передний ступичный подшипн...</td>\n",
       "      <td>перейти содержимое; главный; тот раздел; двиг...</td>\n",
       "      <td>симптом последствие звук выйти строй подшипни...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6727</td>\n",
       "      <td>16052</td>\n",
       "      <td>как снять ступица ваза нива</td>\n",
       "      <td>дробный; можно список ниже; вадик чер; войти;...</td>\n",
       "      <td>снимать ступица ваза нива</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7329</td>\n",
       "      <td>16595</td>\n",
       "      <td>как поменять подшипник ступица нива шеврол видео</td>\n",
       "      <td>ремонт автомобиль; видео; выбор; клип; хороши...</td>\n",
       "      <td>как поменять подшипник ступица нива шеврол видео</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7577</td>\n",
       "      <td>16818</td>\n",
       "      <td>замена подшипник передний ступица нива</td>\n",
       "      <td>весь подшипник; справочник; использование; ст...</td>\n",
       "      <td>замена подшипник передний ступица нива являть...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7836</td>\n",
       "      <td>17050</td>\n",
       "      <td>ваза замена подшипник ступица</td>\n",
       "      <td>общий сведение; диагностика неисправность; дв...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7975</td>\n",
       "      <td>17176</td>\n",
       "      <td>как заменить подшипник ступица передний колес...</td>\n",
       "      <td>проверка регулировка зазор подшипник ступица;...</td>\n",
       "      <td>как заменить подшипник ступица передний колес...</td>\n",
       "      <td>как заменить подшипник ступица передний колес...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "6110  15498               простой разборка ступица нива ремонт   \n",
       "6239  15613       снятие установка подшипник ступица нива нива   \n",
       "6405  15763   замена регулировка передний ступичный подшипн...   \n",
       "6727  16052                        как снять ступица ваза нива   \n",
       "7329  16595   как поменять подшипник ступица нива шеврол видео   \n",
       "7577  16818             замена подшипник передний ступица нива   \n",
       "7836  17050                      ваза замена подшипник ступица   \n",
       "7975  17176   как заменить подшипник ступица передний колес...   \n",
       "\n",
       "                                                   refs  \\\n",
       "6110   ваза; войти; авто; бизнес; дом; интернет; мед...   \n",
       "6239   главный; ваза; ремонт; тюнинг; тест драйв; ха...   \n",
       "6405   перейти содержимое; главный; тот раздел; двиг...   \n",
       "6727   дробный; можно список ниже; вадик чер; войти;...   \n",
       "7329   ремонт автомобиль; видео; выбор; клип; хороши...   \n",
       "7577   весь подшипник; справочник; использование; ст...   \n",
       "7836   общий сведение; диагностика неисправность; дв...   \n",
       "7975   проверка регулировка зазор подшипник ступица;...   \n",
       "\n",
       "                                                   desc  \\\n",
       "6110                                                      \n",
       "6239   подшипник ступица лада амортизатор шаровой оп...   \n",
       "6405   симптом последствие звук выйти строй подшипни...   \n",
       "6727                          снимать ступица ваза нива   \n",
       "7329   как поменять подшипник ступица нива шеврол видео   \n",
       "7577   замена подшипник передний ступица нива являть...   \n",
       "7836                                                      \n",
       "7975   как заменить подшипник ступица передний колес...   \n",
       "\n",
       "                                               keywords  \n",
       "6110                                                     \n",
       "6239   подвеска колесо нива пыльник рычаг амортизато...  \n",
       "6405                                                     \n",
       "6727                                                     \n",
       "7329                                                     \n",
       "7577                                                     \n",
       "7836                                                     \n",
       "7975   как заменить подшипник ступица передний колес...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.id.isin(same_theme_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#можно для каждой группы посчитать такую штуку, потом взять n минимальных расстояний \n",
    "#только желательно ещё как-нибудь графически проверить, что это работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#надо как-то снизить размерность в этой матрице, а потом посчитать расстояния между векторами\n",
    "#внутри пространства меньшей размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<90x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 588 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mtx_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "dist_inside = pairwise_distances(tfidf_mtx, metric= 'cosine')\n",
    "dist_inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994144981916966"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_inside[15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9744234883487349"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_inside.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4, 15, 20, 37, 84, 86, 89], dtype=int64),)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.asarray(traingroups[19])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25215142803799906"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_inside[dist_inside > 0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = np.asarray(traingroups[5])[np.where(np.asarray(traingroups[5])[:, 1])][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96467697, 0.95193392, 0.92133704,\n",
       "       0.96666667, 0.96666667, 0.96666667, 0.96276687, 0.96438714,\n",
       "       0.96666667, 0.95994812, 0.96438702, 0.96666667, 0.96666667,\n",
       "       0.96252717, 0.96666667, 0.91763767, 0.92497215, 0.96666667,\n",
       "       0.96666667, 0.95762439, 0.96109081, 0.95310484, 0.96666667,\n",
       "       0.96276687, 0.96666667, 0.96438702, 0.96666667, 0.95368406])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_inside.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##End of Песочница"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 19770)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mtx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чистовик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_machine = decomposition.TruncatedSVD(n_components=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d0709e2df4893800421e75cd29ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "isoutlier = {}\n",
    "f1_list = []\n",
    "for group in tqdm(traingroups):\n",
    "    indices = np.asarray(traingroups[group])[:, 0]\n",
    "    tfidf_mtx = vectorizer.fit_transform(df[df.id.isin(indices)].title.values)\n",
    "    pca_machine = decomposition.TruncatedSVD(n_components=int(np.sqrt(tfidf_mtx.shape[1])))#\n",
    "    X_pca = pca_machine.fit_transform(tfidf_mtx)\n",
    "    clustering = DBSCAN(eps=0.8, min_samples=2, metric='cosine').fit(X_pca)\n",
    "    isout = list(np.asarray(clustering.labels_ != -1, dtype = int))\n",
    "    f1 = f1_score(np.asarray(traingroups[group])[:, 1], np.asarray(isout))\n",
    "    f1_list.append(f1)\n",
    "    ind_list = list(indices)\n",
    "    for i in range(len(ind_list)):\n",
    "        isoutlier[(group, ind_list[i])] = isout[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3985839482657143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(np.asarray(traingroups[1])[:, 1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(traingroups[group])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccar(s1, s2):\n",
    "    u = len(s1.union(s2))\n",
    "    if u > 0:\n",
    "        return len(s1.intersection(s2))/u\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s):\n",
    "    #temp = s.strip(' \\t\\n!()#*?:;,.').split()\n",
    "    #return [morph.parse(word)[0].normal_form for word in s.split() if len(word) >= 3]\n",
    "    #return [i.strip('.,:').lower() for i in temp if len(i) >= 3]\n",
    "    return s.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749e053f72294e749ee3e14aa3dd1682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(11690, 54) (11690,) (11690,)\n",
      "Wall time: 10min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in tqdm(traingroups_data):\n",
    "    docs = traingroups_data[new_group]\n",
    "    for k, (doc_id, title, refs, keywords, desc, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        dist = {'title':[], 'keywords':[], 'desc':[], 'title_jaccar':[], 'refs_jaccar':[]}\n",
    "        #title_dist = []\n",
    "        #jaccars = []\n",
    "        #extra_dist = []\n",
    "        words = set(bag_of_words(title))\n",
    "        wdesc = set(bag_of_words(desc))\n",
    "        wkey = set(bag_of_words(keywords))\n",
    "        temp = [i.strip().split() for i in refs.split(';') if len(i) >= 3]\n",
    "        wrefs = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) >= 3])\n",
    "        #wrefs = set(temp)\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, refs_j, keywords_j, desc_j, target_j = docs[j]\n",
    "            words_j = set(bag_of_words(title_j))\n",
    "            dist['title'].append(len(words.intersection(words_j)))\n",
    "            ###\n",
    "            wdesc_j = set(bag_of_words(desc_j))\n",
    "            wkey_j = set(bag_of_words(keywords_j))\n",
    "            temp = [i.strip().split() for i in refs_j.split(';') if len(i) >= 3]\n",
    "            #wrefs_j = set(temp)\n",
    "            wrefs_j = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) >= 3])\n",
    "            #jaccars.extend([jaccar(wdesc_j, wdesc), jaccar(wrefs_j, wrefs), jaccar(words, words_j)])\n",
    "            #all_dist.extend([len(wkey.intersection(wkey_j)), len(wdesc.intersection(wdesc_j))])\n",
    "            dist['keywords'].append(len(wkey.intersection(wkey_j)))\n",
    "            dist['desc'].append(len(wdesc.intersection(wdesc_j)))\n",
    "            dist['title_jaccar'].append(jaccar(words, words_j))\n",
    "            dist['refs_jaccar'].append(jaccar(wrefs_j, wrefs))\n",
    "        features = []\n",
    "        for key in ['title', 'keywords', 'desc', 'title_jaccar', 'refs_jaccar']:\n",
    "            if (key == 'keywords') | (key == 'desc'):\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:4])\n",
    "            else:\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:15])\n",
    "        features.append(isoutlier[(new_group, doc_id)])\n",
    "        #features.append(isoutlier_refs[(new_group, doc_id)])\n",
    "        #features = sorted(all_dist, reverse=True)[0:15]\n",
    "        #features.extend(sorted(jaccars, reverse = True)[0:10])\n",
    "        X_train.append(features)\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.        , 4.        , 4.        , ..., 0.12264151, 0.11818182,\n",
       "        1.        ],\n",
       "       [6.        , 6.        , 6.        , ..., 0.15454545, 0.15419501,\n",
       "        1.        ],\n",
       "       [3.        , 3.        , 3.        , ..., 0.08812261, 0.08695652,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.02479339, 0.02424242,\n",
       "        1.        ],\n",
       "       [4.        , 1.        , 1.        , ..., 0.05027933, 0.04977376,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 0.04712939, 0.04585153,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение моделей, подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs = X_train[:, -1]\n",
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.        , 4.        , 4.        , ..., 0.12301587, 0.12264151,\n",
       "        0.11818182],\n",
       "       [6.        , 6.        , 6.        , ..., 0.17236842, 0.15454545,\n",
       "        0.15419501],\n",
       "       [3.        , 3.        , 3.        , ..., 0.09236948, 0.08812261,\n",
       "        0.08695652],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.02491103, 0.02479339,\n",
       "        0.02424242],\n",
       "       [4.        , 1.        , 1.        , ..., 0.05111821, 0.05027933,\n",
       "        0.04977376],\n",
       "       [1.        , 1.        , 1.        , ..., 0.04979253, 0.04712939,\n",
       "        0.04585153]])"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если добавляем признак от dbscan \n",
    "X_train = scaler.fit_transform(X_train[:,:-1])\n",
    "X_train = np.hstack([X_train, dbs.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#без dbscan\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06940584,  0.91559146,  1.08864629, ...,  0.83060644,\n",
       "         0.87650367,  0.85084107],\n",
       "       [ 1.54456183,  1.98490204,  2.21208159, ...,  1.66093309,\n",
       "         1.42517507,  1.48486643],\n",
       "       [ 0.11909385,  0.38093618,  0.52692864, ...,  0.31499945,\n",
       "         0.28286125,  0.30110838],\n",
       "       ...,\n",
       "       [-1.30637414, -1.22302969, -1.15822432, ..., -0.81994794,\n",
       "        -0.8062495 , -0.8029961 ],\n",
       "       [ 0.59424984, -0.6883744 , -0.59650667, ..., -0.37902807,\n",
       "        -0.36795239, -0.35350773],\n",
       "       [-0.83121814, -0.6883744 , -0.59650667, ..., -0.40133183,\n",
       "        -0.42212378, -0.42255997]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d4c7a10908417fb3d0e6ccdd214561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    for C in np.linspace(0.001, 0.5, 10):\n",
    "        #[0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\n",
    "        clf = LogisticRegression(C = C, solver='lbfgs')\n",
    "        clf.fit(X_train_temp, y_train_temp)\n",
    "        prediction = clf.predict(X_val)\n",
    "        f1 = f1_score(y_val, prediction)\n",
    "        fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "        fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "        if C not in parameters_f1:\n",
    "            parameters_f1[C] = []\n",
    "        parameters_f1[C].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001     , 0.05644444, 0.11188889, 0.16733333, 0.22277778,\n",
       "       0.27822222, 0.33366667, 0.38911111, 0.44455556, 0.5       ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.001, 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]]})\n",
    "    temp['C'] = key\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f_beta_001</th>\n",
       "      <th>f_beta_100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.593599</td>\n",
       "      <td>0.696090</td>\n",
       "      <td>0.525109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.056444</td>\n",
       "      <td>0.602881</td>\n",
       "      <td>0.690197</td>\n",
       "      <td>0.542585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.111889</td>\n",
       "      <td>0.603463</td>\n",
       "      <td>0.688180</td>\n",
       "      <td>0.545244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.603383</td>\n",
       "      <td>0.686174</td>\n",
       "      <td>0.546436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.222778</td>\n",
       "      <td>0.603845</td>\n",
       "      <td>0.686263</td>\n",
       "      <td>0.547173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.278222</td>\n",
       "      <td>0.603080</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.546255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.333667</td>\n",
       "      <td>0.602848</td>\n",
       "      <td>0.685197</td>\n",
       "      <td>0.546360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.389111</td>\n",
       "      <td>0.602013</td>\n",
       "      <td>0.684557</td>\n",
       "      <td>0.545332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.444556</td>\n",
       "      <td>0.602059</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.545841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.602199</td>\n",
       "      <td>0.683445</td>\n",
       "      <td>0.546402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1  f_beta_001  f_beta_100\n",
       "C                                         \n",
       "0.001000  0.593599    0.696090    0.525109\n",
       "0.056444  0.602881    0.690197    0.542585\n",
       "0.111889  0.603463    0.688180    0.545244\n",
       "0.167333  0.603383    0.686174    0.546436\n",
       "0.222778  0.603845    0.686263    0.547173\n",
       "0.278222  0.603080    0.685879    0.546255\n",
       "0.333667  0.602848    0.685197    0.546360\n",
       "0.389111  0.602013    0.684557    0.545332\n",
       "0.444556  0.602059    0.683700    0.545841\n",
       "0.500000  0.602199    0.683445    0.546402"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby('C').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa3ed059aec4869b8723f06276c5e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "thresholds = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    #for C in np.linspace(0.001, 0.5, 10):\n",
    "        #[0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict(X_val)\n",
    "    for threshold in np.linspace(0.2, 0.4, 5):\n",
    "    #[0.25, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        prediction_t = np.asarray(prediction >= threshold, dtype = int)\n",
    "        f1 = f1_score(y_val, prediction_t)\n",
    "        fbeta_1 = fbeta_score(y_val, prediction_t, beta = 0.001)\n",
    "        fbeta_2 = fbeta_score(y_val, prediction_t, beta = 100)\n",
    "        if threshold not in thresholds:\n",
    "            thresholds[threshold] = []\n",
    "        thresholds[threshold].append((f1, fbeta_1, fbeta_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70969072, 0.60329002, 0.33830763, ..., 0.15215779, 0.3859822 ,\n",
       "       0.50616722])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_beta001 >> precision Cbest = 0.001\n",
    "f_bea100 >> recall Cbest = 0.11\n",
    "f1 Cbest = 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedd8d434ccd4a878b95d46313dd12c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    clf = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict_proba(X_val)[:, 1]\n",
    "    for threshold in np.linspace(0.2, 0.4, 5):\n",
    "    #[0.25, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        prediction_t = np.asarray(prediction >= threshold, dtype = int)\n",
    "        f1 = f1_score(y_val, prediction_t)\n",
    "        fbeta_1 = fbeta_score(y_val, prediction_t, beta = 0.001)\n",
    "        fbeta_2 = fbeta_score(y_val, prediction_t, beta = 100)\n",
    "        if threshold not in thresholds:\n",
    "            thresholds[threshold] = []\n",
    "        thresholds[threshold].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in thresholds:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in thresholds[key]], \n",
    "                                'f_beta_001': [i[1] for i in thresholds[key]], \n",
    "                                'f_beta_100': [i[2] for i in thresholds[key]]})\n",
    "    temp['t'] = key\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t\n",
       "0.20    0.620410\n",
       "0.25    0.638207\n",
       "0.30    0.646114\n",
       "0.35    0.650938\n",
       "0.40    0.638985\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby('t').f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t\n",
       "0.30    0.562189\n",
       "0.35    0.603824\n",
       "0.40    0.638875\n",
       "0.45    0.665975\n",
       "0.50    0.686761\n",
       "Name: f_beta_001, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby('t').f_beta_001.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 t_best = 0.3\n",
    "f_beta_001 0.7\n",
    "fbeta_100 0.25 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3 , 0.35, 0.4 , 0.45, 0.5 ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.3, 0.5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c6bd2b619e45ce84341ba3186ea89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#thresholds = {}\n",
    "parameters_f1 = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    clf = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict(X_val)\n",
    "    #prediction_t = np.asarray(prediction >= 0.4, dtype = int)\n",
    "    f1 = f1_score(y_val, prediction)\n",
    "    fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "    fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "    if i not in parameters_f1:\n",
    "        parameters_f1[i] = []\n",
    "    parameters_f1[i].append((f1, fbeta_1, fbeta_2))\n",
    "    #print(f1, fbeta_1, fbeta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]]})\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f_beta_001</th>\n",
       "      <th>f_beta_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.587135</td>\n",
       "      <td>0.697783</td>\n",
       "      <td>0.511549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.095719</td>\n",
       "      <td>0.110881</td>\n",
       "      <td>0.096934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.366102</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.288782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.576832</td>\n",
       "      <td>0.638743</td>\n",
       "      <td>0.508033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.584958</td>\n",
       "      <td>0.664335</td>\n",
       "      <td>0.525871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.796392</td>\n",
       "      <td>0.575856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.845394</td>\n",
       "      <td>0.610686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1  f_beta_001  f_beta_100\n",
       "count  9.000000    9.000000    9.000000\n",
       "mean   0.587135    0.697783    0.511549\n",
       "std    0.095719    0.110881    0.096934\n",
       "min    0.366102    0.500000    0.288782\n",
       "25%    0.576832    0.638743    0.508033\n",
       "50%    0.584958    0.664335    0.525871\n",
       "75%    0.605634    0.796392    0.575856\n",
       "max    0.700272    0.845394    0.610686"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_reg: \n",
    "f1 = 0.536412\n",
    "f_001 = 0.696774\n",
    "f_100 = 0.467504    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_features = 75\n",
    "0.534247\n",
    "0.648760\n",
    "0.486080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "модифицированные фичи\n",
    "0.584958\n",
    "0.664335\n",
    "0.525871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " array([ 0,  1,  2,  3,  6,  7,  9, 11, 14, 17, 19, 20, 22, 23, 24, 25, 27,\n",
       "        28, 29, 30, 31, 32, 33, 34, 37, 40, 41, 43, 45, 46, 49, 51],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(clf.coef_ > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21594231,  0.07217693,  0.06750532,  0.16115087, -0.06121074,\n",
       "        -0.01484309,  0.00686427,  0.13741699, -0.11407353,  0.03433764,\n",
       "        -0.28833621,  0.16122478, -0.12715123, -0.63655428,  0.4784934 ,\n",
       "        -0.24546121, -0.02337509,  0.31265803, -0.36687017,  0.05286761,\n",
       "         0.05373897, -0.19614837,  0.39603255,  0.05389279,  0.23336239,\n",
       "         0.05229998, -0.17880639,  0.3953796 ,  0.13572783,  0.20215368,\n",
       "         0.41303241,  0.32493809,  0.21669494,  0.16054196,  0.065898  ,\n",
       "        -0.19789605, -0.46777195,  0.28844181, -0.03582818, -0.00794133,\n",
       "         0.03944739,  0.02435624, -0.30201212,  0.37881475, -0.48120549,\n",
       "         0.07068799,  0.23552486, -0.16372671, -0.32021719,  0.45777862,\n",
       "        -0.00820402,  0.35736528, -0.20031004]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931b50862ae54b28b1565dab92358169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    for m_depth in range(2, 11):\n",
    "    #[5, 10, 15, 20, 25, 50, 100]:\n",
    "        #[0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\n",
    "        for m_features in range(2,8):\n",
    "        #range(1, 10, 2):\n",
    "            clf = RandomForestClassifier(max_depth=m_depth, max_features = m_features, n_estimators = 20, random_state=0)\n",
    "            clf.fit(X_train_temp, y_train_temp)\n",
    "            prediction = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, prediction)\n",
    "            fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "            fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "            if (m_depth, m_features) not in parameters_f1:\n",
    "                parameters_f1[(m_depth, m_features)] = []\n",
    "            parameters_f1[(m_depth, m_features)].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]]})\n",
    "    temp['max_depth'] = key[0]\n",
    "    temp['max_features'] = key[1]\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_depth  max_features\n",
       "2          2               0.606690\n",
       "           3               0.608113\n",
       "           4               0.614854\n",
       "           5               0.609334\n",
       "           6               0.612641\n",
       "           7               0.608581\n",
       "3          2               0.611555\n",
       "           3               0.610934\n",
       "           4               0.612026\n",
       "           5               0.623253\n",
       "           6               0.622693\n",
       "           7               0.626111\n",
       "4          2               0.616708\n",
       "           3               0.620500\n",
       "           4               0.618155\n",
       "           5               0.636206\n",
       "           6               0.640560\n",
       "           7               0.635501\n",
       "5          2               0.632411\n",
       "           3               0.631837\n",
       "           4               0.631695\n",
       "           5               0.643441\n",
       "           6               0.638967\n",
       "           7               0.641540\n",
       "6          2               0.626132\n",
       "           3               0.639935\n",
       "           4               0.636573\n",
       "           5               0.642971\n",
       "           6               0.638262\n",
       "           7               0.634479\n",
       "7          2               0.630284\n",
       "           3               0.642424\n",
       "           4               0.634734\n",
       "           5               0.645075\n",
       "           6               0.640150\n",
       "           7               0.641494\n",
       "8          2               0.640510\n",
       "           3               0.647239\n",
       "           4               0.646079\n",
       "           5               0.640369\n",
       "           6               0.638704\n",
       "           7               0.638000\n",
       "9          2               0.641592\n",
       "           3               0.638192\n",
       "           4               0.643594\n",
       "           5               0.640084\n",
       "           6               0.642115\n",
       "           7               0.643992\n",
       "10         2               0.636930\n",
       "           3               0.642570\n",
       "           4               0.637503\n",
       "           5               0.646887\n",
       "           6               0.645630\n",
       "           7               0.642654\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby(['max_depth', 'max_features']).f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features = 4 \n",
    "max_depth = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#попробуем улучшить качество за счёт комбинации моделей: дадим СЛ вероятность, предсказанную логрегрессией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95cbda6cebc4a498eb54185c554d9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1070,) (10620, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1208,) (10482, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1248,) (10442, 53)\n",
      "(1163,) (10527, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1155,) (10535, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1219,) (10471, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1058,) (10632, 53)\n",
      "(1292,) (10398, 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1239,) (10451, 53)\n",
      "\n",
      "(10652, 55) (11690, 53) (10652,) (10652,)\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "new_groups = []\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    logreg.fit(X_train_temp, y_train_temp)\n",
    "    prediction = logreg.predict_proba(X_val)[:, 1]\n",
    "    prediction = (prediction - np.mean(prediction))/np.std(prediction)\n",
    "    print(prediction.shape, X_train_temp.shape)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train_temp, y_train_temp)\n",
    "    #prediction = linreg.predict(X_val)\n",
    "    prediction_t = np.asarray(linreg.predict(X_val) >= 0.35, dtype = int)\n",
    "    X_val = np.hstack([X_val, prediction.reshape(-1,1)])\n",
    "    X_val = np.hstack([X_val, prediction_t.reshape(-1,1)])\n",
    "    if i == 1:\n",
    "        X_new_train = X_val\n",
    "        y_new_train = y_val\n",
    "    else:\n",
    "        X_new_train = np.concatenate([X_new_train, X_val])\n",
    "        y_new_train = np.concatenate([y_new_train, y_val])\n",
    "    new_groups.extend(groups_train[np.where((groups_train < i + 13) & (groups_train >= i))])\n",
    "new_groups = np.asarray(new_groups)\n",
    "print(X_new_train.shape, X_train.shape, new_groups.shape, y_new_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f52a200db94b87b811db756638ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "#m_depth = 9\n",
    "#m_features = 9\n",
    "for i in tqdm(range(1, max(new_groups) - 13, 13)):\n",
    "    X_train_temp = X_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    y_train_temp = y_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    X_val = X_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    y_val = y_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    for m_depth in range(8, 15):\n",
    "        for m_features in range(8,15):\n",
    "            clf = RandomForestClassifier(max_depth=m_depth, max_features = m_features, n_estimators = 50, random_state=0)\n",
    "            clf.fit(X_train_temp, y_train_temp)\n",
    "            prediction = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, prediction)\n",
    "            fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "            fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "            if (m_depth, m_features) not in parameters_f1:\n",
    "                parameters_f1[(m_depth, m_features)] = []\n",
    "            parameters_f1[(m_depth, m_features)].append((f1, fbeta_1, fbeta_2))\n",
    "    #if i not in parameters_f1:\n",
    "        #parameters_f1[i] = []\n",
    "    #parameters_f1[i].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]],\n",
    "                                'depth': key[0], 'features': key[1]})\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth = 9\n",
    "max_features = 9\n",
    "Для модели с обеими регрессиями: \n",
    "max_depth = 10\n",
    "features = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depth  features\n",
       "8      8           0.643813\n",
       "       9           0.637187\n",
       "       10          0.643284\n",
       "       11          0.641022\n",
       "       12          0.636477\n",
       "       13          0.638436\n",
       "       14          0.637765\n",
       "9      8           0.643946\n",
       "       9           0.638547\n",
       "       10          0.641216\n",
       "       11          0.637104\n",
       "       12          0.636180\n",
       "       13          0.635672\n",
       "       14          0.627386\n",
       "10     8           0.649657\n",
       "       9           0.632968\n",
       "       10          0.636930\n",
       "       11          0.638275\n",
       "       12          0.640627\n",
       "       13          0.640476\n",
       "       14          0.636151\n",
       "11     8           0.648004\n",
       "       9           0.642812\n",
       "       10          0.638876\n",
       "       11          0.632249\n",
       "       12          0.642055\n",
       "       13          0.634267\n",
       "       14          0.641224\n",
       "12     8           0.642297\n",
       "       9           0.641385\n",
       "       10          0.638163\n",
       "       11          0.634383\n",
       "       12          0.632304\n",
       "       13          0.631773\n",
       "       14          0.639476\n",
       "13     8           0.634431\n",
       "       9           0.636707\n",
       "       10          0.629926\n",
       "       11          0.633814\n",
       "       12          0.640765\n",
       "       13          0.633504\n",
       "       14          0.635012\n",
       "14     8           0.634969\n",
       "       9           0.634835\n",
       "       10          0.634487\n",
       "       11          0.635015\n",
       "       12          0.635419\n",
       "       13          0.636122\n",
       "       14          0.632956\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby(['depth', 'features']).f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF_plus_logreg: \n",
    "f1 = 0.556453 \n",
    "f_001 = 0.669921 \n",
    "f_100 = 0.506657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_features = 75\n",
    "0.543851\n",
    "0.650065\n",
    "0.489387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "модифицированные фичи\n",
    "0.612736\n",
    "0.691176\n",
    "0.540276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с нормализацией: \n",
    "0.627775\t\n",
    "0.674735\t\n",
    "0.592245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с dbscan:\n",
    "0.627503\t\n",
    "0.688778\t\n",
    "0.585291\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с dbscan, убирая ненужные фичи\n",
    "0.632101\t\n",
    "0.693050\t\n",
    "0.594130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, финальное:\n",
    "0.632405\t\n",
    "0.695853\t\n",
    "0.593312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00719565, 0.00690649, 0.0337205 , 0.06424077, 0.02772767,\n",
       "       0.01398973, 0.0177307 , 0.00576894, 0.00389543, 0.00414922,\n",
       "       0.00161034, 0.00157841, 0.00198815, 0.00210286, 0.00259267,\n",
       "       0.00451139, 0.0045441 , 0.00404249, 0.00348913, 0.00705629,\n",
       "       0.00733547, 0.00627251, 0.00724933, 0.01123955, 0.01620462,\n",
       "       0.04334572, 0.10315461, 0.06134109, 0.06488339, 0.05021611,\n",
       "       0.0373135 , 0.02016186, 0.02528253, 0.01671422, 0.0190518 ,\n",
       "       0.0189992 , 0.01381047, 0.02821467, 0.01395126, 0.01229206,\n",
       "       0.01145668, 0.01040584, 0.01127642, 0.00956985, 0.01128578,\n",
       "       0.01050416, 0.00917577, 0.0106528 , 0.0096914 , 0.01120926,\n",
       "       0.01081618, 0.01109828, 0.01119753, 0.02708185, 0.0387033 ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 14, 24, 25, 26, 27, 28, 29,\n",
       "        30, 31, 32, 33, 34, 35, 36, 37, 40, 44, 46, 47, 48, 49, 50, 51, 53],\n",
       "       dtype=int64),)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(clf.feature_importances_ > 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'title' 0-14\n",
    "'keywords' 15-18\n",
    "'desc' 19-22\n",
    "'title_jaccar' 23-38\n",
    "'refs_jaccar' 38-52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f7004fa010440dbab973e3806c6a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    clf = RandomForestClassifier(max_depth=6, max_features = 4, n_estimators = 50, random_state=0)\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, prediction)\n",
    "    fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "    fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "    if (m_depth, m_features) not in parameters_f1:\n",
    "        parameters_f1[(m_depth, m_features)] = []\n",
    "    parameters_f1[(m_depth, m_features)].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]]})\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f_beta_001</th>\n",
       "      <th>f_beta_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.628841</td>\n",
       "      <td>0.679399</td>\n",
       "      <td>0.590772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.088443</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.097371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.455128</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.379692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.611268</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.550910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.586209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.626996</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.668722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.763124</td>\n",
       "      <td>0.849003</td>\n",
       "      <td>0.693685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1  f_beta_001  f_beta_100\n",
       "count  9.000000    9.000000    9.000000\n",
       "mean   0.628841    0.679399    0.590772\n",
       "std    0.088443    0.096628    0.097371\n",
       "min    0.455128    0.568000    0.379692\n",
       "25%    0.611268    0.607143    0.550910\n",
       "50%    0.622449    0.645833    0.586209\n",
       "75%    0.626996    0.725564    0.668722\n",
       "max    0.763124    0.849003    0.693685"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF:\n",
    "f1 = 0.588800\n",
    "f_001 = 0.624161\n",
    "f_100 = 0.569663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_features = 75\n",
    "0.558036\n",
    "0.637097\n",
    "0.538706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "0.622449\n",
    "0.645833\n",
    "0.586209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4,  5,  6,  7, 10, 12, 14, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
       "        33, 34, 35, 36, 37], dtype=int64),)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(clf.feature_importances_ > 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'title' 0-15\n",
    "'keywords' 15-30\n",
    "'desc' 30-45\n",
    "'title_jaccar' 45-60\n",
    "'refs_jaccar' 60-75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#попробуем наоборот: в логрегрессию добавить фичу из RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560632222e1042ac9cd85f7d7b7060e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(10652, 54) (11690, 53) (10652,) (10652,)\n"
     ]
    }
   ],
   "source": [
    "new_groups = []\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    rf = RandomForestClassifier(max_depth=6, max_features = 4, n_estimators = 50, random_state=0)\n",
    "    #LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    rf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = rf.predict_proba(X_val)[:, 1]\n",
    "    prediction = (prediction - np.mean(prediction))/np.std(prediction)\n",
    "    #print(prediction.shape, X_train_temp.shape)\n",
    "    X_val = np.hstack([X_val, prediction.reshape(-1,1)])\n",
    "    if i == 1:\n",
    "        X_new_train = X_val\n",
    "        y_new_train = y_val\n",
    "    else:\n",
    "        X_new_train = np.concatenate([X_new_train, X_val])\n",
    "        y_new_train = np.concatenate([y_new_train, y_val])\n",
    "    new_groups.extend(groups_train[np.where((groups_train < i + 13) & (groups_train >= i))])\n",
    "new_groups = np.asarray(new_groups)\n",
    "print(X_new_train.shape, X_train.shape, new_groups.shape, y_new_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02126564, 0.00886717, 0.01657372, 0.01294358, 0.01975668,\n",
       "       0.01839428, 0.01475759, 0.00622943, 0.0175407 , 0.02316212,\n",
       "       0.00240566, 0.00227449, 0.00216672, 0.00188952, 0.00211137,\n",
       "       0.00232909, 0.0012974 , 0.00527419, 0.00230982, 0.00134805,\n",
       "       0.00443018, 0.00890003, 0.00342641, 0.00788249, 0.00229498,\n",
       "       0.00851338, 0.00213468, 0.00274811, 0.0073893 , 0.00315202,\n",
       "       0.03362576, 0.07061407, 0.05947146, 0.07742325, 0.07775566,\n",
       "       0.06153821, 0.09769653, 0.1095822 , 0.054124  , 0.01746112,\n",
       "       0.00381174, 0.0059545 , 0.01014695, 0.00602419, 0.00648585,\n",
       "       0.00980972, 0.01704717, 0.01857936, 0.00951828, 0.01956118])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ddede356b4cdc9eac21359499bdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters_f1 = {}\n",
    "for i in tqdm(range(1, max(new_groups) - 13, 13)):\n",
    "    X_train_temp = X_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    y_train_temp = y_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    X_val = X_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    y_val = y_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    clf = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, prediction)\n",
    "    fbeta_1 = fbeta_score(y_val, prediction, beta = 0.001)\n",
    "    fbeta_2 = fbeta_score(y_val, prediction, beta = 100)\n",
    "    if (m_depth, m_features) not in parameters_f1:\n",
    "        parameters_f1[(m_depth, m_features)] = []\n",
    "    parameters_f1[(m_depth, m_features)].append((f1, fbeta_1, fbeta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in parameters_f1:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in parameters_f1[key]], \n",
    "                                'f_beta_001': [i[1] for i in parameters_f1[key]], \n",
    "                                'f_beta_100': [i[2] for i in parameters_f1[key]]})\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f_beta_001</th>\n",
       "      <th>f_beta_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.582924</td>\n",
       "      <td>0.710532</td>\n",
       "      <td>0.498330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.084814</td>\n",
       "      <td>0.123239</td>\n",
       "      <td>0.078079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.396104</td>\n",
       "      <td>0.504132</td>\n",
       "      <td>0.326215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.576107</td>\n",
       "      <td>0.637052</td>\n",
       "      <td>0.476620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.585508</td>\n",
       "      <td>0.701686</td>\n",
       "      <td>0.537540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.631601</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.545326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.672389</td>\n",
       "      <td>0.873605</td>\n",
       "      <td>0.555353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1  f_beta_001  f_beta_100\n",
       "count  8.000000    8.000000    8.000000\n",
       "mean   0.582924    0.710532    0.498330\n",
       "std    0.084814    0.123239    0.078079\n",
       "min    0.396104    0.504132    0.326215\n",
       "25%    0.576107    0.637052    0.476620\n",
       "50%    0.585508    0.701686    0.537540\n",
       "75%    0.631601    0.810476    0.545326\n",
       "max    0.672389    0.873605    0.555353"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logreg plus RF:\n",
    "f1 = 0.557222\n",
    "f_001 = 0.664319\n",
    "f_100 = 0.481038  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logreg_plus_RF:\n",
    "0.585508\n",
    "0.701686\n",
    "0.537540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#А теперь в линрегрессию добавим RF и logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60795aedad5f41ec940a63e4833062de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(10652, 55) (11690, 53) (10652,) (10652,)\n"
     ]
    }
   ],
   "source": [
    "new_groups = []\n",
    "for i in tqdm(range(1, len(traingroups_data) - 13, 13)):\n",
    "    X_train_temp = X_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    y_train_temp = y_train[np.where((groups_train >= i + 13) | (groups_train < i))]\n",
    "    X_val = X_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    y_val = y_train[np.where((groups_train < i + 13) & (groups_train >= i))]\n",
    "    rf = RandomForestClassifier(max_depth=8, max_features = 4, n_estimators = 100, random_state=0)\n",
    "    #LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    rf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = rf.predict_proba(X_val)[:, 1]\n",
    "    prediction = (prediction - np.mean(prediction))/np.std(prediction)\n",
    "    #print(prediction.shape, X_train_temp.shape)\n",
    "    logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "    logreg.fit(X_train_temp, y_train_temp)\n",
    "    lr_prediction = logreg.predict_proba(X_val)[:, 1]\n",
    "    lr_prediction = (lr_prediction - np.mean(lr_prediction))/np.std(lr_prediction)\n",
    "    X_val = np.hstack([X_val, prediction.reshape(-1,1)])\n",
    "    X_val = np.hstack([X_val, lr_prediction.reshape(-1,1)])\n",
    "    if i == 1:\n",
    "        X_new_train = X_val\n",
    "        y_new_train = y_val\n",
    "    else:\n",
    "        X_new_train = np.concatenate([X_new_train, X_val])\n",
    "        y_new_train = np.concatenate([y_new_train, y_val])\n",
    "    new_groups.extend(groups_train[np.where((groups_train < i + 13) & (groups_train >= i))])\n",
    "new_groups = np.asarray(new_groups)\n",
    "print(X_new_train.shape, X_train.shape, new_groups.shape, y_new_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1de8352aa84cd6a464c28f4d9ea2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = {}\n",
    "for i in tqdm(range(1, max(new_groups) - 13, 13)):\n",
    "    X_train_temp = X_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    y_train_temp = y_new_train[np.where((new_groups >= i + 13) | (new_groups < i))]\n",
    "    X_val = X_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    y_val = y_new_train[np.where((new_groups < i + 13) & (new_groups >= i))]\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_temp, y_train_temp)\n",
    "    prediction = clf.predict(X_val)\n",
    "    for threshold in np.linspace(0.2, 0.4, 5):\n",
    "    #[0.25, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        prediction_t = np.asarray(prediction >= threshold, dtype = int)\n",
    "        f1 = f1_score(y_val, prediction_t)\n",
    "        fbeta_1 = fbeta_score(y_val, prediction_t, beta = 0.001)\n",
    "        fbeta_2 = fbeta_score(y_val, prediction_t, beta = 100)\n",
    "        if threshold not in thresholds:\n",
    "            thresholds[threshold] = []\n",
    "        thresholds[threshold].append((f1, fbeta_1, fbeta_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame()\n",
    "for key in thresholds:\n",
    "    temp = pd.DataFrame(data = {'f1': [i[0] for i in thresholds[key]], \n",
    "                                'f_beta_001': [i[1] for i in thresholds[key]], \n",
    "                                'f_beta_100': [i[2] for i in thresholds[key]]})\n",
    "    temp['t'] = key\n",
    "    f_df = f_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t\n",
       "0.20    0.636001\n",
       "0.25    0.652704\n",
       "0.30    0.666479\n",
       "0.35    0.673385\n",
       "0.40    0.672568\n",
       "Name: f1, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.groupby('t').f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22790709,  0.10321245,  0.08623716,  0.36014026, -0.2674196 ,\n",
       "        -0.08509554, -0.04558981,  0.02453484,  0.02690781, -0.00126455,\n",
       "        -0.24397046,  0.11793768, -0.31272112, -0.56976581,  0.33142466,\n",
       "        -0.15858302, -0.11953984,  0.22349863, -0.20873341,  0.07636935,\n",
       "         0.00418748, -0.0961881 ,  0.28118025,  0.02161557,  0.22598039,\n",
       "         0.04898137, -0.24923682,  0.21212416,  0.18962795,  0.22171734,\n",
       "         0.53637036,  0.30481721,  0.04398986,  0.10775096,  0.09157603,\n",
       "        -0.04886054, -0.2913942 ,  0.26113382, -0.0339621 ,  0.01805931,\n",
       "        -0.00327944, -0.01492795, -0.28974463,  0.64893381, -0.57970492,\n",
       "         0.00509042,  0.09168318, -0.0816365 , -0.21084195,  0.39186683,\n",
       "         0.09175215, -0.03366557,  0.0645571 ,  0.32080901]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest c добавлением фичи \"вероятность 1, предсказанная логрегрессией\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_data = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = df[df.id == doc_id].title.values\n",
    "    refs = df[df.id == doc_id].refs.values\n",
    "    keywords = df[df.id == doc_id].keywords.values\n",
    "    desc = df[df.id == doc_id].desc.values\n",
    "    if doc_group not in testgroups_data:\n",
    "        testgroups_data[doc_group] = []\n",
    "    testgroups_data[doc_group].append((doc_id, title[0], refs[0], keywords[0], desc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440197cca5924e108447312f2bb23221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "isoutlier_t = {}\n",
    "f1_list = []\n",
    "for group in tqdm(list(test_data.group_id.drop_duplicates())):\n",
    "    indices = np.asarray(test_data[test_data.group_id == group].doc_id)\n",
    "    tfidf_mtx = vectorizer.fit_transform(df[df.id.isin(indices)].title.values)\n",
    "    pca_machine = decomposition.TruncatedSVD(n_components=int(np.sqrt(tfidf_mtx.shape[1])))#\n",
    "    X_pca = pca_machine.fit_transform(tfidf_mtx)\n",
    "    clustering = DBSCAN(eps=0.9, min_samples=2, metric='cosine').fit(X_pca)\n",
    "    isout = list(np.asarray(clustering.labels_ != -1, dtype = int))\n",
    "    ind_list = list(indices)\n",
    "    for i in range(len(ind_list)):\n",
    "        isoutlier_t[(group, ind_list[i])] = isout[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b913025def434b0f9df8c5cce833c879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(16627, 53)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = []\n",
    "\n",
    "for new_group in tqdm(testgroups_data):\n",
    "    docs = testgroups_data[new_group]\n",
    "    for k, (doc_id, title, refs, keywords, desc) in enumerate(docs):\n",
    "        dist = {'title':[], 'keywords':[], 'desc':[], 'title_jaccar':[], 'refs_jaccar':[]}\n",
    "        words = set(bag_of_words(title))\n",
    "        wdesc = set(bag_of_words(desc))\n",
    "        wkey = set(bag_of_words(keywords))\n",
    "        temp = [i.strip().split() for i in refs.split(';') if len(i) > 4]\n",
    "        wrefs = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) > 3])\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, refs_j, keywords_j, desc_j = docs[j]\n",
    "            words_j = set(bag_of_words(title_j))\n",
    "            dist['title'].append(len(words.intersection(words_j)))\n",
    "            wdesc_j = set(bag_of_words(desc_j))\n",
    "            wkey_j = set(bag_of_words(keywords_j))\n",
    "            temp = [i.strip().split() for i in refs_j.split(';') if len(i) > 4]\n",
    "            wrefs_j = set([item.strip() for sublist in temp for item in sublist if len(item.strip()) > 3])\n",
    "            dist['keywords'].append(len(wkey.intersection(wkey_j)))\n",
    "            dist['desc'].append(len(wdesc.intersection(wdesc_j)))\n",
    "            dist['title_jaccar'].append(jaccar(words, words_j))\n",
    "            dist['refs_jaccar'].append(jaccar(wrefs_j, wrefs))\n",
    "        features = []\n",
    "        for key in ['title', 'keywords', 'desc', 'title_jaccar', 'refs_jaccar']:\n",
    "            if (key == 'keywords') | (key == 'desc'):\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:4])\n",
    "            else:\n",
    "                features.extend(sorted(dist[key], reverse=True)[0:15])\n",
    "        #features.append(isoutlier_t[(new_group, doc_id)])\n",
    "        X_test.append(features)\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.74935843, 2.28751069, 2.06193328, 1.90872541, 1.79110351,\n",
       "       1.69332763, 1.60402053, 1.53182207, 1.47031651, 1.41471343,\n",
       "       1.36338751, 1.31950385, 1.27536356, 1.23977759, 1.20325064,\n",
       "       2.09384089, 1.56347305, 1.32497861, 1.15697177, 3.52882806,\n",
       "       2.66834902, 2.27673225, 2.05611634, 0.36641613, 0.28867971,\n",
       "       0.25250163, 0.22689557, 0.20742181, 0.19286504, 0.18067785,\n",
       "       0.17045521, 0.16236647, 0.15440535, 0.14737262, 0.14154812,\n",
       "       0.13587097, 0.13099963, 0.12641718, 0.24498966, 0.16365994,\n",
       "       0.13623564, 0.12020569, 0.11038912, 0.10173871, 0.09620581,\n",
       "       0.09074958, 0.0872766 , 0.08288553, 0.08018009, 0.07705643,\n",
       "       0.0736467 , 0.07167489, 0.0698533 ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbs = X_test[:,-1]\n",
    "X_test = scaler.transform(X_test)\n",
    "#X_test = np.hstack([X_test, dbs.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11909385,  0.38093618,  0.52692864, ...,  0.30919608,\n",
       "         0.33078213,  0.3706948 ],\n",
       "       [ 0.11909385,  0.38093618,  0.52692864, ...,  0.67591061,\n",
       "         0.71760732,  0.76037873],\n",
       "       [ 1.06940584,  0.38093618,  0.52692864, ...,  0.39437527,\n",
       "         0.39553911,  0.42071003],\n",
       "       ...,\n",
       "       [ 0.11909385,  0.38093618,  0.52692864, ...,  0.20302988,\n",
       "         0.21295928,  0.14942923],\n",
       "       [ 0.59424984,  0.91559146,  1.08864629, ..., -0.04678112,\n",
       "        -0.01390856,  0.01783139],\n",
       "       [ 3.44518581,  4.12352319,  4.45895219, ...,  0.13125161,\n",
       "         0.15907721,  0.17302501]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "prediction = logreg.predict_proba(X_test)[:, 1]\n",
    "prediction = (prediction - np.mean(prediction))/np.std(prediction)\n",
    "rf = RandomForestClassifier(max_depth=8, max_features = 4, n_estimators = 100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_prediction = rf.predict_proba(X_test)[:, 1]\n",
    "rf_prediction = (rf_prediction - np.mean(rf_prediction))/np.std(rf_prediction)\n",
    "X_test = np.hstack([X_test, prediction.reshape(-1,1)])\n",
    "X_test = np.hstack([X_test, rf_prediction.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 53)"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5845.0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:int(X_train.shape[0]/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray([1,2,3])\n",
    "a[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 0.11, solver='lbfgs')\n",
    "x1 = X_train[:int(X_train.shape[0]/2), :]\n",
    "y1 = y_train[:int(X_train.shape[0]/2)]\n",
    "x2 = X_train[int(X_train.shape[0]/2):, :]\n",
    "y2 = y_train[int(X_train.shape[0]/2):]\n",
    "logreg.fit(x1, y1)\n",
    "pred1 = logreg.predict_proba(x2)[:, 1]\n",
    "#pred1 = (pred1 - np.mean(pred1))/np.std(pred1)\n",
    "logreg.fit(x2, y2)\n",
    "pred2 = logreg.predict_proba(x1)[:, 1]\n",
    "#pred2 = (pred2 - np.mean(pred2))/np.std(pred2)\n",
    "pred = np.concatenate([pred2, pred1])\n",
    "pred = (pred - np.mean(pred))/np.std(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, pred.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=8, max_features = 4, n_estimators = 100, random_state=0)\n",
    "x1 = X_train[:int(X_train.shape[0]/2), :]\n",
    "y1 = y_train[:int(X_train.shape[0]/2)]\n",
    "x2 = X_train[int(X_train.shape[0]/2):, :]\n",
    "y2 = y_train[int(X_train.shape[0]/2):]\n",
    "rf.fit(x1, y1)\n",
    "pred1 = rf.predict_proba(x2)[:, 1]\n",
    "#pred1 = (pred1 - np.mean(pred1))/np.std(pred1)\n",
    "rf.fit(x2, y2)\n",
    "pred2 = rf.predict_proba(x1)[:, 1]\n",
    "#pred2 = (pred2 - np.mean(pred2))/np.std(pred2)\n",
    "pred = np.concatenate([pred2, pred1])\n",
    "pred = (pred - np.mean(pred))/np.std(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, pred.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690,)"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 55)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 55)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier(max_depth=6, max_features = 4, n_estimators = 50, random_state=0)\n",
    "#clf = RandomForestClassifier(max_depth=9, max_features = 9, n_estimators = 50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred = clf.predict(X_test)\n",
    "finalpred_t = np.asarray(finalpred >= 0.375, dtype = int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred = pd.DataFrame(data = {'pair_id':test_data.pair_id, 'target': finalpred_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5389"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalpred_t[finalpred_t == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred.to_csv('linear_rf_log__comb_submission.сsv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#А если просто RandomForest\n",
    "clf = RandomForestClassifier(max_depth=6, max_features = 4, n_estimators = 50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=6, max_features=4, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train[:,:-1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred = clf.predict(X_test[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred = pd.DataFrame(data = {'pair_id':test_data.pair_id, 'target': finalpred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred.to_csv('rf_submission.сsv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
